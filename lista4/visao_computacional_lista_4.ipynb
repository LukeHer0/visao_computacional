{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27816bbc",
   "metadata": {},
   "source": [
    "# LISTA 4 DE VISÃO COMPUTACIONAL\n",
    "# LUCAS HERON SANTOS ANCHIETA\n",
    "# RUAN TENÓRIO DE MELO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(images, titles = None):\n",
    "  n = len(images)\n",
    "  plt.figure(figsize=(20, 10))\n",
    "  for i in range(n):\n",
    "    plt.subplot(1, n, i + 1)\n",
    "    plt.imshow(images[i])\n",
    "    if titles is not None:\n",
    "      plt.title(titles[i])\n",
    "    plt.axis('off')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7857c4b2",
   "metadata": {},
   "source": [
    "## Questão 1 \n",
    "As fotografias em modo retrato se popularizaram nos  últimos anos. Elas consistem em segmentar foreground e background em uma fotografia, e borrar o background, simulando o efeito de uma câmera DLSR, como na imagem localizada em (./img/q1-camera.jpg).\n",
    "\n",
    "O objetivo dessa questão é simular este efeito, usando mapas de disparidade gerados por duas imagens. Para um tutorial de como gerar mapas de disparidade usando OpenCV, veja https://docs.opencv.org/4.7.0/dd/d53/tutorial_py_depthmap.html. Após a aquisição do mapa de disparidade, você deve usá-lo adequadamente para detectar o foreground, aplicar um filtro gaussiano para borrar apenas o background, e combinar as duas partes para obter um efeito como na figura acima. Experimente seu algoritmo em três pares de imagens, em cenários diferentes, adquiridas através de pequenas variações de ponto de vista da câmera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgL = cv2.imread('./img/controller-left.jpg', 0)\n",
    "imgR = cv2.imread('./img/controller-right.jpg', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ply_header = '''ply\n",
    "format ascii 1.0\n",
    "element vertex %(vert_num)d\n",
    "property float x\n",
    "property float y\n",
    "property float z\n",
    "property uchar red\n",
    "property uchar green\n",
    "property uchar blue\n",
    "end_header\n",
    "'''\n",
    "\n",
    "def write_ply(fn, verts, colors):\n",
    "    verts = verts.reshape(-1, 3)\n",
    "    colors = colors.reshape(-1, 3)\n",
    "    verts = np.hstack([verts, colors])\n",
    "    with open(fn, 'wb') as f:\n",
    "        f.write((ply_header % dict(vert_num=len(verts))).encode('utf-8'))\n",
    "        np.savetxt(f, verts, fmt='%f %f %f %d %d %d ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading images...\n",
      "computing disparity...\n",
      "generating 3d point cloud...\n",
      "out.ply saved\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "print('loading images...')\n",
    "imgL = cv2.pyrDown(cv2.imread(\"./img/controller-left.jpg\")) \n",
    "imgR = cv2.pyrDown(cv2.imread(\"./img/controller-right.jpg\"))\n",
    "\n",
    "# disparity range is tuned for 'aloe' image pair\n",
    "window_size = 3\n",
    "min_disp = 16\n",
    "num_disp = 112-min_disp\n",
    "stereo = cv2.StereoSGBM_create(minDisparity = min_disp,\n",
    "    numDisparities = num_disp,\n",
    "    blockSize = 16,\n",
    "    P1 = 8*3*window_size**2,\n",
    "    P2 = 32*3*window_size**2,\n",
    "    disp12MaxDiff = 1,\n",
    "    uniquenessRatio = 10,\n",
    "    speckleWindowSize = 100,\n",
    "    speckleRange = 32\n",
    ")\n",
    "\n",
    "print('computing disparity...')\n",
    "disp = stereo.compute(imgR, imgL).astype(np.float32) / 16.0\n",
    "\n",
    "print('generating 3d point cloud...',)\n",
    "h, w = imgL.shape[:2]\n",
    "f = 0.8*w                          # guess for focal length\n",
    "Q = np.float32([[1, 0, 0, -0.5*w],\n",
    "                [0,-1, 0,  0.5*h], # turn points 180 deg around x-axis,\n",
    "                [0, 0, 0,     -f], # so that y-axis looks up\n",
    "                [0, 0, 1,      0]])\n",
    "points = cv2.reprojectImageTo3D(disp, Q)\n",
    "colors = cv2.cvtColor(imgL, cv2.COLOR_BGR2RGB)\n",
    "mask = disp > disp.min()\n",
    "out_points = points[mask]\n",
    "out_colors = colors[mask]\n",
    "out_fn = 'out.ply'\n",
    "write_ply(out_fn, out_points, out_colors)\n",
    "print('%s saved' % out_fn)\n",
    "\n",
    "cv2.imshow('left', imgL)\n",
    "cv2.imshow('disparity', (disp-min_disp)/num_disp)\n",
    "cv2.waitKey()\n",
    "\n",
    "print('Done')\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef4c216",
   "metadata": {},
   "source": [
    "## Questão 2 \n",
    "Leia o seguinte tutorial sobre reconstrução 3d, até a etapa de geração do arquivo PLY: https://medium.com/analytics-vidhya/depth-sensing-and-3d-reconstruction-512ed121aa60\n",
    "\n",
    "a) Experimente o código disponibilizado no tutorial em três exemplos distintos, gerando a nuvem de pontos e visualizando com o Meshlab.\n",
    "\n",
    "b) Descreva, com suas palavras, as etapas realizadas desde o fornecimento do par de imagens de entrada, até a geração da nuvem de pontos em formato PLY."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ad0fdb",
   "metadata": {},
   "source": [
    "## Questão 3 \n",
    "Usando a plataforma Kaggle, pesquise um base supervisionada de imagens que possa ser usada para treinar um classificador binário de imagens. Esta base deve conter exemplos de duas classes. Considere 70% das amostras para treino, e 30% para teste (método holdout).\n",
    "\n",
    "a) Usando OpenCV, extraia descritores HOG (Histogram of Gradients), e treine um classificador SVM. Exiba a acurácia atingida no conjunto de teste e a matriz de confusão. Exiba exemplos. Obs.: note que essa abordagem é semelhante ao detector Dalal-Triggs, porém aqui estamos usando-a para classificação de imagens. \n",
    "\n",
    "b) Agora o objetivo é treinar uma CNN. Usando a abordagem de transferência de aprendizado (transfer learning), realize treinamentos usando modelos VGG16, ResNet50, e MobileNetV2 pré-treinados. Compare os resultados usando acurácias e matrizes de confusão. Exiba exemplos de acerto e erro."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668e8a0c",
   "metadata": {},
   "source": [
    "## Questão 4 \n",
    "O objetivo dessa questão é criar um contador de veículos em vídeos de uma rodovia. Você deve experimentar um modelo pré-treinado da YOLO, e um Single Shot MultiBox Detection (SSD).\n",
    "\n",
    "a) Estude a SSD, e descreva suas principais diferenças em relação à YOLO.\n",
    "\n",
    "b) Para cada um dos dois modelos, realize as detecções de veículos em cada frame do vídeo em anexo (classroom). Gere os vídeos com as bounding boxes detectadas.\n",
    "\n",
    "c) Para cada um dos dois modelos, plote um gráfico onde o eixo horizontal é o tempo (quadro do vídeo), e o vertical é a quantidade de carros detectada pelo modelo."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
