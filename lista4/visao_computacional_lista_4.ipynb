{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27816bbc",
   "metadata": {},
   "source": [
    "# Lista 4 de Visão Computacional\n",
    "Alunos: LUCAS HERON SANTOS ANCHIETA RUAN TENÓRIO DE MELO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8458421f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 00:00:33.664344: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-07 00:00:33.675476: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746586833.690216   27235 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746586833.694923   27235 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1746586833.705498   27235 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746586833.705516   27235 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746586833.705517   27235 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746586833.705518   27235 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-07 00:00:33.708814: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e073fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(images, titles = None, cmap=None):\n",
    "  n = len(images)\n",
    "  plt.figure(figsize=(20, 10))\n",
    "  for i in range(n):\n",
    "    plt.subplot(1, n, i + 1)\n",
    "    plt.imshow(images[i], cmap=cmap)\n",
    "    if titles is not None:\n",
    "      plt.title(titles[i])\n",
    "    plt.axis('off')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7857c4b2",
   "metadata": {},
   "source": [
    "## Questão 1 \n",
    "As fotografias em modo retrato se popularizaram nos  últimos anos. Elas consistem em segmentar foreground e background em uma fotografia, e borrar o background, simulando o efeito de uma câmera DLSR, como na imagem localizada em (./img/q1-camera.jpg).\n",
    "\n",
    "O objetivo dessa questão é simular este efeito, usando mapas de disparidade gerados por duas imagens. Para um tutorial de como gerar mapas de disparidade usando OpenCV, veja https://docs.opencv.org/4.7.0/dd/d53/tutorial_py_depthmap.html. Após a aquisição do mapa de disparidade, você deve usá-lo adequadamente para detectar o foreground, aplicar um filtro gaussiano para borrar apenas o background, e combinar as duas partes para obter um efeito como na figura acima. Experimente seu algoritmo em três pares de imagens, em cenários diferentes, adquiridas através de pequenas variações de ponto de vista da câmera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcae3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 3\n",
    "# num_disp = 112-min_disp\n",
    "num_disp = 16 * 6\n",
    "block_size = 9\n",
    "def get_disparity_map(imgL, imgR, min_disp=0):\n",
    "    stereo = cv2.StereoSGBM_create(minDisparity = min_disp,\n",
    "        numDisparities = num_disp,\n",
    "        blockSize = block_size,\n",
    "        P1 = 8 * 3 * window_size**2,\n",
    "        P2 = 32 * 3 * window_size**2,\n",
    "        disp12MaxDiff = 1,\n",
    "        uniquenessRatio = 10,\n",
    "        speckleWindowSize = 100,\n",
    "        speckleRange = 32\n",
    "    )\n",
    "   \n",
    "    disp = stereo.compute(imgL, imgR).astype(np.float32) / 16.0\n",
    "    # disp_normalized = cv2.normalize(disp, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX)\n",
    "    disparity_map_bm_raw = stereo.compute(imgL, imgR)\n",
    "    valid_mask = disparity_map_bm_raw >= 0\n",
    "    disparity_to_normalize = np.where(valid_mask, disparity_map_bm_raw, 0)\n",
    "\n",
    "    # # Normaliza apenas os valores válidos para 0-255 para visualização e thresholding simples\n",
    "    disparity_map_normalized = cv2.normalize(disparity_to_normalize, None,\n",
    "                                             alpha=0, beta=255,\n",
    "                                             norm_type=cv2.NORM_MINMAX,\n",
    "                                             dtype=cv2.CV_8U)\n",
    "    return disparity_map_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc53517",
   "metadata": {},
   "outputs": [],
   "source": [
    "def blurry_bg_with_disparity_map(imgL, disparity_map, threshold=70):\n",
    "    ret, foreground_mask = cv2.threshold(disparity_map,\n",
    "                                         threshold,\n",
    "                                         255,\n",
    "                                         cv2.THRESH_BINARY)\n",
    "    foreground_mask = foreground_mask.astype(np.uint8)\n",
    "    imgL_blurred = cv2.GaussianBlur(imgL, (55, 55), 0)\n",
    "\n",
    "    foreground_mask_3ch = cv2.cvtColor(foreground_mask, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "    show_images([imgL, imgL_blurred, foreground_mask_3ch], ['Original Image', 'Blurred Image', 'Foreground Mask'], cmap='gray')\n",
    "\n",
    "    mask_boolean = foreground_mask_3ch.astype(bool)\n",
    "\n",
    "    output_image = np.where(mask_boolean, imgL, imgL_blurred)\n",
    "    imgL = cv2.cvtColor(imgL, cv2.COLOR_BGR2RGB)\n",
    "    imgL_blurred = cv2.cvtColor(imgL_blurred, cv2.COLOR_BGR2RGB)\n",
    "    output_image = cv2.cvtColor(output_image, cv2.COLOR_BGR2RGB)\n",
    "    show_images([imgL, imgL_blurred, output_image], ['Original Image', 'Blurred Image', 'Output Image'], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86229d64",
   "metadata": {},
   "source": [
    "### Exemplo 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e967af",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgL = cv2.imread('./img/aloeL.jpg')\n",
    "imgL_gray = cv2.cvtColor(imgL, cv2.COLOR_BGR2GRAY)\n",
    "imgR = cv2.imread('./img/aloeR.jpg')\n",
    "imgR_gray = cv2.cvtColor(imgR, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "disp = get_disparity_map(imgL_gray, imgR_gray, 16)\n",
    "show_images([imgL_gray, disp], ['Left Image',  'Disparity Map'], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50590f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "blurry_bg_with_disparity_map(imgL, disp, 160)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4925787d",
   "metadata": {},
   "source": [
    "### Exemplo 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0af299e",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgL = cv2.imread('./img/monopolyL.png')\n",
    "imgL_gray = cv2.cvtColor(imgL, cv2.COLOR_BGR2GRAY)\n",
    "imgR = cv2.imread('./img/monopolyR.png')\n",
    "imgR_gray = cv2.cvtColor(imgR, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "disp = get_disparity_map(imgL_gray, imgR_gray)\n",
    "\n",
    "show_images([imgL_gray, disp], ['Left Image',  'Disparity Map'], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a86549",
   "metadata": {},
   "outputs": [],
   "source": [
    "blurry_bg_with_disparity_map(imgL,  disp, 79)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aac1f98",
   "metadata": {},
   "source": [
    "### Exemplo 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0189e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgL = cv2.imread('./img/babyL.png')\n",
    "imgL_gray = cv2.cvtColor(imgL, cv2.COLOR_BGR2GRAY)\n",
    "imgR = cv2.imread('./img/babyR.png')\n",
    "imgR_gray = cv2.cvtColor(imgR, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "disp = get_disparity_map(imgL_gray, imgR_gray)\n",
    "show_images([imgL_gray, imgR_gray,disp], ['Left Image', \"Right Image\", 'Disparity Map'], cmap='gray')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445311ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "blurry_bg_with_disparity_map(imgL, disp, 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef4c216",
   "metadata": {},
   "source": [
    "## Questão 2 \n",
    "Leia o seguinte tutorial sobre reconstrução 3d, até a etapa de geração do arquivo PLY: https://medium.com/analytics-vidhya/depth-sensing-and-3d-reconstruction-512ed121aa60"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0a7203",
   "metadata": {},
   "source": [
    "### a) Experimente o código disponibilizado no tutorial em três exemplos distintos, gerando a nuvem de pontos e visualizando com o Meshlab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9499bde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_ply(fn, verts, colors):\n",
    "    ply_header = '''ply\n",
    "    format ascii 1.0\n",
    "    element vertex %(vert_num)d\n",
    "    property float x\n",
    "    property float y\n",
    "    property float z\n",
    "    property uchar red\n",
    "    property uchar green\n",
    "    property uchar blue\n",
    "    end_header\n",
    "    '''\n",
    "    out_colors = colors.copy()\n",
    "    verts = verts.reshape(-1, 3)\n",
    "    verts = np.hstack([verts, out_colors])\n",
    "    with open(fn, 'wb') as f:\n",
    "        f.write((ply_header % dict(vert_num=len(verts))).encode('utf-8'))\n",
    "        np.savetxt(f, verts, fmt='%f %f %f %d %d %d ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ecd00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder_left = \"data_road/training/image_2/\"\n",
    "data_folder_right = \"data_road/training_right/image_3/\"\n",
    "data_folder_calib = \"data_road/training/calib/\"\n",
    "cat = ['uu', 'uum', 'um']\n",
    "IDX_LEN = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575955b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_num = 1\n",
    "cat_idx = 2\n",
    "fname = cat[cat_idx]+'_'+str(idx_num).zfill(IDX_LEN)\n",
    "img_fname = fname + '.png'\n",
    "calib_fname = fname + '.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0e42fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_left_color = cv2.imread(data_folder_left + img_fname)\n",
    "img_right_color = cv2.imread(data_folder_right + img_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f4d5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_left_bw = cv2.blur(cv2.cvtColor(img_left_color, cv2.COLOR_RGB2GRAY),(5,5))\n",
    "img_right_bw = cv2.blur(cv2.cvtColor(img_right_color, cv2.COLOR_RGB2GRAY),(5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e319ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "stereo = cv2.StereoBM_create(numDisparities=96, blockSize=11)\n",
    "disparity = stereo.compute(img_left_bw,img_right_bw)\n",
    "\n",
    "img = disparity.copy()\n",
    "plt.imshow(img, 'CMRmap_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd8fa35",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_type_1 = 'P2'\n",
    "matrix_type_2 = 'P3'\n",
    "\n",
    "calib_file = data_folder_calib + calib_fname\n",
    "with open(calib_file, 'r') as f:\n",
    "    fin = f.readlines()\n",
    "    for line in fin:\n",
    "        if line[:2] == matrix_type_1:\n",
    "            calib_matrix_1 = np.array(line[4:].strip().split(\" \")).astype('float32').reshape(3,-1)\n",
    "        elif line[:2] == matrix_type_2:\n",
    "            calib_matrix_2 = np.array(line[4:].strip().split(\" \")).astype('float32').reshape(3,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981898e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cam1 = calib_matrix_1[:,:3] # left image - P2\n",
    "cam2 = calib_matrix_2[:,:3] # right image - P3\n",
    "\n",
    "Tmat = np.array([0.54, 0., 0.])\n",
    "\n",
    "rev_proj_matrix = np.zeros((4,4))\n",
    "\n",
    "cv2.stereoRectify(cameraMatrix1 = cam1,cameraMatrix2 = cam2, \\\n",
    "                  distCoeffs1 = 0, distCoeffs2 = 0, \\\n",
    "                  imageSize = img_left_color.shape[:2], \\\n",
    "                  R = np.identity(3), T = Tmat, \\\n",
    "                  R1 = None, R2 = None, \\\n",
    "                  P1 =  None, P2 =  None, Q = rev_proj_matrix);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a17f7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "points = cv2.reprojectImageTo3D(img, rev_proj_matrix)\n",
    "\n",
    "#reflect on x axis\n",
    "reflect_matrix = np.identity(3)\n",
    "reflect_matrix[0] *= -1\n",
    "points = np.matmul(points,reflect_matrix)\n",
    "\n",
    "#extract colors from image\n",
    "colors = cv2.cvtColor(img_left_color, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "#filter by min disparity\n",
    "mask = img > img.min()\n",
    "out_points = points[mask]\n",
    "out_colors = colors[mask]\n",
    "\n",
    "#filter by dimension\n",
    "idx = np.fabs(out_points[:,0]) < 4.5\n",
    "out_points = out_points[idx]\n",
    "out_colors = out_colors.reshape(-1, 3)\n",
    "out_colors = out_colors[idx]\n",
    "\n",
    "write_ply('out.ply', out_points, out_colors)\n",
    "print('%s saved' % 'out.ply')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06695eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reflected_pts = np.matmul(out_points, reflect_matrix)\n",
    "projected_img,_ = cv2.projectPoints(reflected_pts, np.identity(3), np.array([0., 0., 0.]), \\\n",
    "                          cam2[:3,:3], np.array([0., 0., 0., 0.]))\n",
    "projected_img = projected_img.reshape(-1, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f84a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "blank_img = np.zeros(img_left_color.shape, 'uint8')\n",
    "img_colors = img_right_color[mask][idx].reshape(-1,3)\n",
    "\n",
    "for i, pt in enumerate(projected_img):\n",
    "    pt_x = int(pt[0])\n",
    "    pt_y = int(pt[1])\n",
    "    if pt_x > 0 and pt_y > 0:\n",
    "        # use the BGR format to match the original image type\n",
    "        col = (int(img_colors[i, 2]), int(img_colors[i, 1]), int(img_colors[i, 0]))\n",
    "        cv2.circle(blank_img, (pt_x, pt_y), 1, col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a069d91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images([blank_img], ['Projected Image'], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00680e1f",
   "metadata": {},
   "source": [
    "### b) Descreva, com suas palavras, as etapas realizadas desde o fornecimento do par de imagens de entrada, até a geração da nuvem de pontos em formato PLY."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ad0fdb",
   "metadata": {},
   "source": [
    "## Questão 3 \n",
    "Usando a plataforma Kaggle, pesquise um base supervisionada de imagens que possa ser usada para treinar um classificador binário de imagens. Esta base deve conter exemplos de duas classes. Considere 70% das amostras para treino, e 30% para teste (método holdout)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb8e3bf",
   "metadata": {},
   "source": [
    "### a) Usando OpenCV, extraia descritores HOG (Histogram of Gradients), e treine um classificador SVM. Exiba a acurácia atingida no conjunto de teste e a matriz de confusão. Exiba exemplos. Obs.: note que essa abordagem é semelhante ao detector Dalal-Triggs, porém aqui estamos usando-a para classificação de imagens. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb3584e",
   "metadata": {},
   "source": [
    "### b) Agora o objetivo é treinar uma CNN. Usando a abordagem de transferência de aprendizado (transfer learning), realize treinamentos usando modelos VGG16, ResNet50, e MobileNetV2 pré-treinados. Compare os resultados usando acurácias e matrizes de confusão. Exiba exemplos de acerto e erro."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668e8a0c",
   "metadata": {},
   "source": [
    "## Questão 4 \n",
    "O objetivo dessa questão é criar um contador de veículos em vídeos de uma rodovia. Você deve experimentar um modelo pré-treinado da YOLO, e um Single Shot MultiBox Detection (SSD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b61ee0",
   "metadata": {},
   "source": [
    "### a) Estude a SSD, e descreva suas principais diferenças em relação à YOLO."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95fdad3",
   "metadata": {},
   "source": [
    "### b) Para cada um dos dois modelos, realize as detecções de veículos em cada frame do vídeo em anexo (classroom). Gere os vídeos com as bounding boxes detectadas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7680458",
   "metadata": {},
   "source": [
    "#### YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d89c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO('yolov8n.pt')\n",
    "video = \"./img/bridge.mp4\"\n",
    "cap = cv2.VideoCapture(video)\n",
    "\n",
    "output = \"./img/bridge_out_yolo.mp4\"\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output, fourcc, fps, (width, height))\n",
    "\n",
    "while cap.isOpened():   \n",
    "    ret, frame = cap.read()\n",
    "    if not ret: # If the video has ended, break the loop\n",
    "        break\n",
    "\n",
    "    results = model(frame, classes = [2,3,5])\n",
    "    annotated_frame = results[0].plot()\n",
    "\n",
    "    out.write(annotated_frame)\n",
    "\n",
    "    # cv2.imshow('YOLOv8 Detection', annotated_frame)\n",
    "    # if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "    #     break\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single Shot MultiBox Detection (SSD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model_ssd = cv2.dnn.readNetFromTensorflow(\"./models/ssd_mobilenet_v2_coco_2018_03_29/frozen_inference_graph.pb\",\n",
    "                                          \"./models/ssd_mobilenet_v2_coco_2018_03_29/model.pbtxt\")\n",
    "video = \"./img/bridge.mp4\"\n",
    "output = \"./img/bridge_out_yolo.mp4\"\n",
    "\n",
    "CLASSES = { 3: \"carro\", 6: \"truck\"}\n",
    "\n",
    "cap = cv2.VideoCapture(video)\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FsRAME_HEIGHT))\n",
    "out = cv2.VideoWriter(\"./img/bridge_out_ssd.mp4\", fourcc, fps, (width, height))\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        breaks\n",
    "    rows = frame.shape[0]\n",
    "    cols = frame.shape[1]\n",
    "    blob = cv2.dnn.blobFromImage(frame, size=(300, 300), swapRB=True)\n",
    "    model_ssd.setInput(blob)\n",
    "    cvOut = model_ssd.forward()\n",
    "\n",
    "    for detection in cvOut[0,0,:,:]:\n",
    "        score = float(detection[2])\n",
    "        if score > 0.5:\n",
    "            left = detection[3] * cols\n",
    "            top = detection[4] * rows\n",
    "            right = detection[5] * cols\n",
    "            bottom = detection[6] * rows\n",
    "            class_id = int(detection[1])\n",
    "            if class_id in CLASSES:\n",
    "                label = f\"{CLASSES[class_id]}: {score:.2f}\"\n",
    "                cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), thickness=2)\n",
    "                cv2.putText(frame, label, (int(left), int(top) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "           \n",
    "    cv2.namedWindow(\"frame\", cv2.WINDOW_NORMAL)\n",
    "    cv2.imshow(\"frame\", frame)\n",
    "    if cv2.waitKey(1) >= 0:  # Break with ESC \n",
    "        break\n",
    "    out.write(frame)\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1beac17b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow._api.v2.compat.v2.io.gfile' has no attribute 'FastGFile'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mv2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgfile\u001b[49m\u001b[43m.\u001b[49m\u001b[43mFastGFile\u001b[49m(\u001b[33m'\u001b[39m\u001b[33m./models/ssd_mobilenet_v3_large_coco_2020_01_14/frozen_inference_graph.pb\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m      2\u001b[39m     graph_def = tf.compat.v2.GraphDef()\n\u001b[32m      3\u001b[39m     graph_def.ParseFromString(f.read())\n",
      "\u001b[31mAttributeError\u001b[39m: module 'tensorflow._api.v2.compat.v2.io.gfile' has no attribute 'FastGFile'"
     ]
    }
   ],
   "source": [
    "with tf.compat.v2.io.gfile.FastGFile('./models/ssd_mobilenet_v3_large_coco_2020_01_14/frozen_inference_graph.pb', 'rb') as f:\n",
    "    graph_def = tf.compat.v2.GraphDef()\n",
    "    graph_def.ParseFromString(f.read())\n",
    "\n",
    "with tf.compat.v2.Session() as sess:\n",
    "    # Restore session\n",
    "    sess.graph.as_default()\n",
    "    tf.import_graph_def(graph_def, name='')\n",
    "\n",
    "    cap = cv2.VideoCapture(\"./img/bridge.mp4\")\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(\"./img/bridge_out_ssd_tf.mp4\", fourcc, fps, (width, height))\n",
    "    while cap.isOpened():\n",
    "        # Read and preprocess an image.\n",
    "        ret, img = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        img = cv2.resize(img, (300, 300))\n",
    "        rows = img.shape[0]\n",
    "        cols = img.shape[1]\n",
    "        inp = cv.resize(img, (300, 300))\n",
    "        inp = inp[:, :, [2, 1, 0]]  # BGR2RGB\n",
    "\n",
    "        # Run the model\n",
    "        inference = sess.run([sess.graph.get_tensor_by_name('num_detections:0'),\n",
    "                        sess.graph.get_tensor_by_name('detection_scores:0'),\n",
    "                        sess.graph.get_tensor_by_name('detection_boxes:0'),\n",
    "                        sess.graph.get_tensor_by_name('detection_classes:0')],\n",
    "                    feed_dict={'image_tensor:0': inp.reshape(1, inp.shape[0], inp.shape[1], 3)})\n",
    "\n",
    "        # Visualize detected bounding boxes.\n",
    "        num_detections = int(inference[0][0])\n",
    "        for i in range(num_detections):\n",
    "            classId = int(inference[3][0][i])\n",
    "            score = float(inference[1][0][i])\n",
    "            bbox = [float(v) for v in inference[2][0][i]]\n",
    "            if score > 0.3:\n",
    "                x = bbox[1] * cols\n",
    "                y = bbox[0] * rows\n",
    "                right = bbox[3] * cols\n",
    "                bottom = bbox[2] * rows\n",
    "                cv.rectangle(img, (int(x), int(y)), (int(right), int(bottom)), (125, 255, 51), thickness=2)\n",
    "                cv.putText(img, str(classId), (int(x), int(y)), cv.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "                cv.putText(img, str(score), (int(x), int(y) + 20), cv.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "                out.write(img)\n",
    "        cap.release()\n",
    "        out.release()\n",
    "\n",
    "cv.imshow('TensorFlow MobileNet-SSD', img)\n",
    "cv.waitKey()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b32c9d",
   "metadata": {},
   "source": [
    "### c) Para cada um dos dois modelos, plote um gráfico onde o eixo horizontal é o tempo (quadro do vídeo), e o vertical é a quantidade de carros detectada pelo modelo."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
